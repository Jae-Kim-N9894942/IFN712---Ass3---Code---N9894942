{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "909d6246-db4e-4b67-99ff-3082511bf35d",
   "metadata": {},
   "source": [
    "### DistilBERT GN-GloVe Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db70a169-4d78-4fe2-9ce3-c618056d8ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import string\n",
    "import codecs\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010f7d81-490b-44fe-b3e3-8a575e8e9670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the TOKENIZERS_PARALLELISM environment variable to disable parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set a random seed and device\n",
    "torch.backends.cudnn.deterministic = True\n",
    "RANDOM_SEED = 367\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define constants\n",
    "NUM_EPOCHS = 5\n",
    "GN_GLOVE_PATH = '/Users/jaehyunkim/Desktop/gender_bias_lipstick-master/data/embeddings/gn_glove'\n",
    "MAX_SEQ_LENGTH = 128  # Adjust as needed\n",
    "\n",
    "# Define a dictionary to store vocab, wv, and w2i for different spaces\n",
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "# Load gender-specific words from two files: male-specific and female-specific\n",
    "male_specific = []\n",
    "female_specific = []\n",
    "\n",
    "# Load male-specific terms\n",
    "with open('/Users/jaehyunkim/Desktop/gender_bias_lipstick-master/data/lists/male_word_file.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        male_specific.append(line.strip())\n",
    "\n",
    "# Load female-specific terms\n",
    "with open('/Users/jaehyunkim/Desktop/gender_bias_lipstick-master/data/lists/female_word_file.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        female_specific.append(line.strip())\n",
    "\n",
    "# Combine male-specific and female-specific terms into one list\n",
    "gender_specific = male_specific + female_specific\n",
    "\n",
    "# Load GN-GloVe embeddings\n",
    "def load_gn_glove_embeddings(filename):\n",
    "    with open(filename + '.vocab', 'r', encoding='utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "vocab_gn_glove, wv_gn_glove, w2i_gn_glove = load_gn_glove_embeddings(GN_GLOVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a38b374-a665-4fec-8d87-26c396b498cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to your local CSV file and the encoding\n",
    "data_file_path = \"/Users/jaehyunkim/Downloads/BUG-main/data/full_BUG.csv\"\n",
    "encoding = 'utf-8'  # Change to the appropriate encoding if necessary\n",
    "df = pd.read_csv(data_file_path, encoding=encoding)\n",
    "\n",
    "# Map \"stereotype\" column values to 0, 1, -1\n",
    "df['stereotype'] = df['stereotype'].map({0: 0, 1: 1, -1: 2})\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, valid_df, test_df = np.split(df.sample(frac=1, random_state=RANDOM_SEED), [int(.7*len(df)), int(.8*len(df))])\n",
    "\n",
    "# Extract text data and labels for train, validation, and test sets\n",
    "train_texts = train_df['sentence_text'].values\n",
    "valid_texts = valid_df['sentence_text'].values\n",
    "test_texts = test_df['sentence_text'].values\n",
    "\n",
    "# Extract labels for train, validation, and test sets\n",
    "train_labels = train_df['stereotype'].values\n",
    "valid_labels = valid_df['stereotype'].values\n",
    "test_labels = test_df['stereotype'].values\n",
    "\n",
    "# Initialize the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the text data for train, valid, and test sets\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(list(valid_texts), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c89ed2c-2fe0-470b-af0f-a620e461023b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create datasets and data loaders\n",
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = GenderDataset(train_encodings, train_labels)\n",
    "valid_dataset = GenderDataset(valid_encodings, valid_labels)\n",
    "test_dataset = GenderDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3068faa-f9cc-4736-a4b6-69c9e269bebb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the modified cosine similarity function using PyTorch\n",
    "def cosine_similarity(embeddings):\n",
    "    embeddings_norm = torch.nn.functional.normalize(embeddings, dim=-1)\n",
    "    similarity = torch.matmul(embeddings_norm, embeddings_norm.transpose(0, 1))\n",
    "    return similarity\n",
    "\n",
    "# Define the modified compute_cosine_similarity_and_bias function\n",
    "def compute_cosine_similarity_and_bias(embeddings, gender_specific):\n",
    "    # Ensure the dimensions match by trimming or padding GN-GloVe embeddings\n",
    "    max_len = max(len(embeddings), len(gender_specific))\n",
    "    \n",
    "    if len(embeddings) < max_len:\n",
    "        # Pad GN-GloVe embeddings with zeros\n",
    "        embeddings = np.pad(embeddings, ((0, max_len - len(embeddings)), (0, 0)), 'constant')\n",
    "    elif len(embeddings) > max_len:\n",
    "        # Trim GN-GloVe embeddings to match the max length\n",
    "        embeddings = embeddings[:max_len, :]\n",
    "    \n",
    "    # Convert embeddings to PyTorch tensors\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    \n",
    "    # Compute cosine similarity matrix for DistilBERT embeddings\n",
    "    distilbert_cosine_sim_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    gender_bias_results = []\n",
    "    \n",
    "    # Compute gender bias scores for each word in the input\n",
    "    for word in gender_specific:\n",
    "        # Convert word to lowercase for consistency with DistilBERT tokens\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Find the token ID for the word in DistilBERT's vocabulary\n",
    "        token_id = tokenizer.convert_tokens_to_ids(word)\n",
    "        \n",
    "        # Calculate cosine similarity between the word's embedding and all other embeddings\n",
    "        vec = distilbert_cosine_sim_matrix[token_id, :]\n",
    "        \n",
    "        # Sort cosine similarities in descending order\n",
    "        sorted_similarities = sorted(enumerate(vec), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Calculate the number of masculine and feminine neighbors\n",
    "        m = 0\n",
    "        f = 0\n",
    "        for i, sim in sorted_similarities[1:]:  # Exclude the word itself (similarity = 1.0)\n",
    "            neighbor_word = tokenizer.decode([i])\n",
    "            if neighbor_word.lower() == neighbor_word:\n",
    "                if neighbor_word in gender_specific:\n",
    "                    if neighbor_word == 'he' or neighbor_word == 'him':\n",
    "                        m += 1\n",
    "                    elif neighbor_word == 'she' or neighbor_word == 'her':\n",
    "                        f += 1\n",
    "                \n",
    "        gender_bias_results.append((word, m, f))\n",
    "    \n",
    "    return gender_bias_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b234e03d-5455-43e5-b9ea-47cf83704b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modify the train function to use cross-entropy loss for multi-class classification\n",
    "def train_multiclass(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate the loss as the cross-entropy loss for multiclass classification\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "\n",
    "# In the training loop, use cross-entropy loss\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c32b007-d2f1-4b1a-a1ed-a0cea6d7a762",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_multiclass_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "            correct_predictions += (predicted_labels == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions * 100.0\n",
    "    return accuracy\n",
    "\n",
    "# Function to load embeddings with restrictions\n",
    "def load_wo_normalize(space, filename, vocab, wv, w2i, exclude_words=None):\n",
    "    print ('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab_muse = [line.strip() for line in f_embed]\n",
    "    \n",
    "    w2i_muse = {w: i for i, w in enumerate(vocab_muse)}\n",
    "    wv_muse = np.load(filename + '.wv.npy')\n",
    "    \n",
    "    if exclude_words:\n",
    "        filtered_vocab_muse = []\n",
    "        filtered_wv_muse = []\n",
    "        filtered_w2i_muse = {}\n",
    "        \n",
    "        for word, i in w2i_muse.items():\n",
    "            if word.lower() not in exclude_words:  # Exclude specific terms\n",
    "                filtered_vocab_muse.append(word)\n",
    "                filtered_wv_muse.append(wv_muse[i])\n",
    "                filtered_w2i_muse[word] = len(filtered_vocab_muse) - 1\n",
    "        \n",
    "        vocab[space] = filtered_vocab_muse\n",
    "        wv[space] = np.array(filtered_wv_muse)\n",
    "        w2i[space] = filtered_w2i_muse\n",
    "    else:\n",
    "        vocab[space] = vocab_muse\n",
    "        wv[space] = wv_muse\n",
    "        w2i[space] = w2i_muse\n",
    "\n",
    "    print ('done')\n",
    "    \n",
    "# Define a function to check if a string contains a digit\n",
    "def has_digit(s):\n",
    "    return any(char.isdigit() for char in s)\n",
    "\n",
    "def has_punct(s):\n",
    "    return any(char in string.punctuation for char in s)\n",
    "\n",
    "# Function to limit vocabulary with restrictions\n",
    "def limit_vocab(space, exclude=None, vec_len=300, exclude_words=None):\n",
    "    vocab_limited = []\n",
    "    \n",
    "    for w in tqdm(vocab[space][:50000]):\n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        \n",
    "        # Apply restrictions on excluded words\n",
    "        if exclude_words and w in exclude_words:\n",
    "            continue\n",
    "        \n",
    "        vocab_limited.append(w)\n",
    "\n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "\n",
    "    print(\"size of vocabulary:\", len(vocab_limited))\n",
    "\n",
    "    wv_limited = np.zeros((len(vocab_limited), vec_len))\n",
    "    for i, w in enumerate(vocab_limited):\n",
    "        wv_limited[i, :] = wv[space][w2i[space][w], :]\n",
    "\n",
    "    w2i_limited = {w: i for i, w in enumerate(vocab_limited)}\n",
    "\n",
    "    return vocab_limited, wv_limited, w2i_limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22b09928-587a-4d72-a9e5-bb8de9daa318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 1086629.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 47694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spaces of limited vocabulary\n",
    "exclude_words = gender_specific\n",
    "\n",
    "# Modify the load_wo_normalize call to include the exclude_words parameter\n",
    "load_wo_normalize('aft', '/Users/jaehyunkim/Desktop/gender_bias_lipstick-master/data/embeddings/gn_glove', vocab, wv, w2i)\n",
    "\n",
    "# Modify the limit_vocab call to include the exclude_words parameter\n",
    "vocab['limit_aft'], wv['limit_aft'], w2i['limit_aft'] = limit_vocab('aft', exclude=exclude_words, vec_len=300)\n",
    "\n",
    "# Check if the vocabularies match\n",
    "assert(vocab['limit_aft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b3071ad-f34d-4ef5-8d78-df420340ef69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5 | Train Loss: 0.0927 | Valid Accuracy: 98.35%\n",
      "Test Accuracy: 98.34%\n",
      "Epoch: 2/5 | Train Loss: 0.0418 | Valid Accuracy: 98.00%\n",
      "Test Accuracy: 98.18%\n",
      "Epoch: 3/5 | Train Loss: 0.0336 | Valid Accuracy: 98.55%\n",
      "Test Accuracy: 98.18%\n",
      "Epoch: 4/5 | Train Loss: 0.0281 | Valid Accuracy: 98.50%\n",
      "Test Accuracy: 98.16%\n",
      "Epoch: 5/5 | Train Loss: 0.0235 | Valid Accuracy: 98.38%\n",
      "Test Accuracy: 98.13%\n",
      "Total Training Time: 2075.11 min\n"
     ]
    }
   ],
   "source": [
    "# Compute cosine similarity and gender bias for embeddings\n",
    "gender_bias_results_after = compute_cosine_similarity_and_bias(wv['limit_aft'], gender_specific)\n",
    "\n",
    "# Training loop with GenderDataset and data loaders\n",
    "# In the training loop, use cross-entropy loss\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_multiclass(model, train_loader, optimizer, DEVICE)\n",
    "    valid_accuracy = compute_multiclass_accuracy(model, valid_loader, DEVICE)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}/{NUM_EPOCHS} | '\n",
    "          f'Train Loss: {train_loss:.4f} | '\n",
    "          f'Valid Accuracy: {valid_accuracy:.2f}%')\n",
    "\n",
    "    # Calculate and print test accuracy\n",
    "    test_accuracy = compute_multiclass_accuracy(model, test_loader, DEVICE)\n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "print(f'Total Training Time: {(time.time() - start_time) / 60:.2f} min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f42fe2-92ef-4a6e-9767-63751011ee9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
